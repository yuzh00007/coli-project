# AI Detection Project

finetunes a pretrained model to predict if a text is AI generated or not. 
two datasets are available in the code: CHEAT (academic abstract) and Tweepfake (Tweets)


# Project File Structure
```commandline
.
├── classifiers
│ ├── AbstractClassifier.py
│ ├── LLMClassifier.py
│ └── TweetClassifier.py
├── data
│ ├── cheat
│ └── tweepfake
├── main.py
├── runners.py
├── README.md
├── requirements.txt
├── run.sub
├── sic_cluster
│ ├── conda_run.sh
│ ├── environment.yml
│ ├── run.sh
│ ├── setup.sh
│ └── setup.sub
└── utils
    ├── parse_trees.py
    └── utils.py
```

# Environment Setup

for local runs:
```commandline
pip install -r requirements.txt
```

for cluster runs:
```commandline
condor_submit sic_cluster/setup.sub
```
this needs to be run only when the `sic_cluster/environment.yml` file is updated.


# Data Files
this project currently finetunes with two datasets. 
first is [TweepFake](https://www.kaggle.com/datasets/mtesconi/twitter-deep-fake-text) - tweets
generated by multiple different language models, including GPT-2 and GPT-3.
the second is [CHEAT](https://github.com/botianzhe/CHEAT) - a scientific abstract dataset
with three different AI methods (this project only uses the generation files).

since both the datasets do not have easy huggingface dataset access, I have provided
both the raw data and cleaned data that is used to run the program in this repository.

if the clean data exists, it will take precedence over the raw data to skip data
preprocessing. but one set or the other must exist. if instead starting from only raw data files, 
these specific file paths must exist:
```
original cheat dataset
- data/cheat/ieee-init.jsonl
- data/cheat/ieee-chatgpt-generation.jsonl

original tweepfake dataset
- data/tweepfake/test.csv
- data/tweepfake/train.csv
- data/tweepfake/valid.csv

parse tree distribution stats
- data/cheat/bot_abstract_parse_count.pkl
- data/cheat/human_abstract_parse_count.pkl
- data/tweepfake/bot_tweet_parse_count.pkl
- data/tweepfake/human_tweet_parse_count.pkl
```

data processing takes about an hour per dataset on the CS cluster machine 
as described by `run.sub`. bulk of the data processing time is taken up by generating
parse trees for every sentence in the dataset. this process creates serialized files
of all three splits named `train-clean.pkl`, `valid-clean.pkl`, and `test-clean.pkl`.
these are provided to save on compute. however, deleting them will force the program
to redo the processing.

parse tree distribution stats are generated by running the `utils/parse_trees.py`
script (can be done locally). the script generates a file for both human and AI texts - 
however in the  training loop, only the human file is used. AI parses can be used for statistical
analysis. you can edit the script to create a distribution for a different corpus.


# Running Project
to run the baseline (off the shelf), then train the model:

```commandline
python main.py --baseline True
python main.py --epoch 3 --per_device_train_batch_size 8 --learning_rate 5e-5
```

see `sic_cluster/run.sh` file for more examples - or to modify jobs submitted to cluster

to run on the CS department clusters:
```
condor_submit run.sub
```

training and evaluation outputs along with any error logs are located in `sic_cluster/logs/`
if running on the cluster. if running locally, all metrics will be printed out.


# Model Checkpoints
after training, finetuned model weights are saved to the `models/` directory.

# Finetuning Another Dataset
create a new file in [classifiers](classifiers) that inherits from the `LLMClassifier` class.
the only function that must be provided is `preprocess_data()` that returns a 
datasets DatasetDict with train, valid, and test datasets. 

then provide a high level training process in the [runners.py](runners.py) file
and call it in [main.py](main.py). 
