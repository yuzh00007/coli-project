{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf7d45e-b166-4c62-bcb3-930969adcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815ee13-6bf9-4d9b-8fd0-fcbd54953241",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66f4337-be41-4de5-b1c4-76fb8066e8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yzhang/.virtualenvs/coli_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yzhang/.virtualenvs/coli_project/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hello-SimpleAI/chatgpt-detector-roberta\")\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec90ea8-a685-45fa-9162-efee47743c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Human', 'score': 0.0007840888574719429},\n",
       "  {'label': 'ChatGPT', 'score': 0.999215841293335}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generated with chatGPT - almost 100% certain it's AI generated\n",
    "pipe(\n",
    "    \"\"\"Alright, so when you hear \"New York Times Bestseller,\" it's not literally saying there's only one book at the number one spot. Think of it more like a list where multiple books can be at the top, but they're still all considered \"bestsellers.\" The New York Times updates this list regularly, so different books can take the top spot at different times. It's kind of like when you have a bunch of favorite toys, but you can only play with one at a time. So, yeah, when you hear about a bunch of books being \"number one,\" they're all just sharing that top spot at different times.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b247396b-1ea4-495c-9b2f-3a56f5beedc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'Human', 'score': 0.7533059120178223},\n",
       "  {'label': 'ChatGPT', 'score': 0.2466941475868225}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generated with Claude - LLM which is not ChatGPT (which was used to create the HC3 dataset)\n",
    "# even though, I would say the text isn't all too different\n",
    "pipe(\n",
    "    \"\"\"Alright, so here's the deal with those \"New York Times #1 Bestseller\" books. It's not like there's just one single bestselling book that gets the top spot. Nah, they've got different lists for different types of books - fiction, non-fiction, paperbacks, you name it. So a bunch of books can be #1 at the same time, just in their own category. Plus, the list changes every week, so lots of books get a turn at being the top dog. It's all just a big marketing thing, really.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cae2f64-ffe8-4d94-b845-630a8815ad55",
   "metadata": {},
   "source": [
    "the big issue we have here is that the pretrained model is trained only on data from one single source - it doesn't do well with data generated (with prompts) from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d21d10-8754-49a9-b85b-9feb85ec11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(scores: dict):\n",
    "    return max(scores, key=lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c587c40e-b6c0-453d-a966-6d2c54d154d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human', 'ChatGPT', 'Human']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    get_prediction(x)[\"label\"] for x in pipe([\n",
    "        \"\"\"How do you do fellow kids?\"\"\",\n",
    "        \"\"\"Alright, so when you hear \"New York Times Bestseller,\" it's not literally saying there's only one book at the number one spot. Think of it more like a list where multiple books can be at the top, but they're still all considered \"bestsellers.\" The New York Times updates this list regularly, so different books can take the top spot at different times. It's kind of like when you have a bunch of favorite toys, but you can only play with one at a time. So, yeah, when you hear about a bunch of books being \"number one,\" they're all just sharing that top spot at different times.\"\"\",\n",
    "        \"\"\"Alright, so here's the deal with those \"New York Times #1 Bestseller\" books. It's not like there's just one single bestselling book that gets the top spot. Nah, they've got different lists for different types of books - fiction, non-fiction, paperbacks, you name it. So a bunch of books can be #1 at the same time, just in their own category. Plus, the list changes every week, so lots of books get a turn at being the top dog. It's all just a big marketing thing, really.\"\"\",\n",
    "    ])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a4653-cd1b-457d-a027-a0a9b364068b",
   "metadata": {},
   "source": [
    "# Out of the Box Evaluation on TweepFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926d3a35-687e-4374-a6f5-600045969d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataHandler():\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    \n",
    "    def read_csv_file(self, file_path, sep=\";\"):\n",
    "        df = pd.read_csv(file_path, sep=sep)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b512cfa-3e2b-4af9-835f-f81346a32a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imranyebot</td>\n",
       "      <td>YEA now that note GOOD</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zawvrk</td>\n",
       "      <td>Listen to This Charming Man by The Smiths  htt...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zawarbot</td>\n",
       "      <td>wish i can i would be seeing other hoes on the...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ahadsheriffbot</td>\n",
       "      <td>The decade in the significantly easier schedul...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kevinhookebot</td>\n",
       "      <td>\"Theim class=\\\"alignnone size-full wp-image-60...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20707</th>\n",
       "      <td>AINarendraModi</td>\n",
       "      <td>Met on the Abversion of our science for the co...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20708</th>\n",
       "      <td>AINarendraModi</td>\n",
       "      <td>Land for their during the opportunity to the p...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20709</th>\n",
       "      <td>DeepDrumpf</td>\n",
       "      <td>@TayandYou doesn't have a clue. You're right. ...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20710</th>\n",
       "      <td>jaden</td>\n",
       "      <td>Me And My Bestie https://t.co/vPq2iDkWZm</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>kevinhookebot</td>\n",
       "      <td>\"Thead has a generate existing the Sparching f...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20712 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          screen_name                                               text  \\\n",
       "0          imranyebot                             YEA now that note GOOD   \n",
       "1              zawvrk  Listen to This Charming Man by The Smiths  htt...   \n",
       "2            zawarbot  wish i can i would be seeing other hoes on the...   \n",
       "3      ahadsheriffbot  The decade in the significantly easier schedul...   \n",
       "4       kevinhookebot  \"Theim class=\\\"alignnone size-full wp-image-60...   \n",
       "...               ...                                                ...   \n",
       "20707  AINarendraModi  Met on the Abversion of our science for the co...   \n",
       "20708  AINarendraModi  Land for their during the opportunity to the p...   \n",
       "20709      DeepDrumpf  @TayandYou doesn't have a clue. You're right. ...   \n",
       "20710           jaden           Me And My Bestie https://t.co/vPq2iDkWZm   \n",
       "20711   kevinhookebot  \"Thead has a generate existing the Sparching f...   \n",
       "\n",
       "      account.type class_type  \n",
       "0              bot     others  \n",
       "1            human      human  \n",
       "2              bot     others  \n",
       "3              bot     others  \n",
       "4              bot        rnn  \n",
       "...            ...        ...  \n",
       "20707          bot        rnn  \n",
       "20708          bot        rnn  \n",
       "20709          bot        rnn  \n",
       "20710        human      human  \n",
       "20711          bot        rnn  \n",
       "\n",
       "[20712 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh = DataHandler()\n",
    "tweepf_train = dh.read_csv_file(\"data/tweepfake/train.csv\")\n",
    "tweepf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e8c89f-7e19-4b7f-9cb9-9ee7737833ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [screen_name, text, account.type, class_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweepf_train[tweepf_train.class_type.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b498d873-4faf-4888-9f94-6a7979074bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [screen_name, text, account.type, class_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweepf_train[tweepf_train[\"account.type\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58309693-01bf-4b86-a74a-9c8186caefdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>10358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn</td>\n",
       "      <td>3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>markov</td>\n",
       "      <td>3920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-2</td>\n",
       "      <td>3109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  count\n",
       "0   human  10358\n",
       "1     rnn   3325\n",
       "2  markov   3920\n",
       "3   gpt-2   3109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_count = tweepf_train[tweepf_train.class_type==\"human\"].text.count()\n",
    "rnn_count = tweepf_train[tweepf_train.class_type==\"rnn\"].text.count()\n",
    "oth_count = tweepf_train[tweepf_train.class_type==\"others\"].text.count()\n",
    "gpt2_count = tweepf_train[tweepf_train.class_type==\"gpt2\"].text.count()\n",
    "pd.DataFrame({\"type\": [\"human\", \"rnn\", \"markov\", \"gpt-2\"], \"count\": [human_count, rnn_count, oth_count, gpt2_count]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac3fb7f-a09c-420b-8847-a1d7ff1dfc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahadsheriff</td>\n",
       "      <td>TIGHT, TIGHT, TIGHT, YEAH!!! https://t.co/wj3n...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>narendramodi</td>\n",
       "      <td>India has millennia old relations with Oman. W...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jaden</td>\n",
       "      <td>Anxious Teenagers</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JustinTrudeau</td>\n",
       "      <td>Our top priority is keeping Canadians safe. Wi...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imranyebot</td>\n",
       "      <td>nah bro You’re taking sis so much I’m just a g...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>DeepDrumpf</td>\n",
       "      <td>You're going to be even prouder when we don't ...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>jaden</td>\n",
       "      <td>https://t.co/10XkzXDBCf https://t.co/cIUIYWEB45</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>ahadsheriff</td>\n",
       "      <td>2. “Once you take the place of the people who ...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>imranyebot</td>\n",
       "      <td>black will be like a company with them need so...</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>GenePark_GPT2</td>\n",
       "      <td>Guys, I hate Facebook. And this Facebook ad ca...</td>\n",
       "      <td>bot</td>\n",
       "      <td>gpt2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2302 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        screen_name                                               text  \\\n",
       "0       ahadsheriff  TIGHT, TIGHT, TIGHT, YEAH!!! https://t.co/wj3n...   \n",
       "1      narendramodi  India has millennia old relations with Oman. W...   \n",
       "2             jaden                                  Anxious Teenagers   \n",
       "3     JustinTrudeau  Our top priority is keeping Canadians safe. Wi...   \n",
       "4        imranyebot  nah bro You’re taking sis so much I’m just a g...   \n",
       "...             ...                                                ...   \n",
       "2297     DeepDrumpf  You're going to be even prouder when we don't ...   \n",
       "2298          jaden    https://t.co/10XkzXDBCf https://t.co/cIUIYWEB45   \n",
       "2299    ahadsheriff  2. “Once you take the place of the people who ...   \n",
       "2300     imranyebot  black will be like a company with them need so...   \n",
       "2301  GenePark_GPT2  Guys, I hate Facebook. And this Facebook ad ca...   \n",
       "\n",
       "     account.type class_type  \n",
       "0           human      human  \n",
       "1           human      human  \n",
       "2           human      human  \n",
       "3           human      human  \n",
       "4             bot     others  \n",
       "...           ...        ...  \n",
       "2297          bot        rnn  \n",
       "2298        human      human  \n",
       "2299        human      human  \n",
       "2300          bot     others  \n",
       "2301          bot       gpt2  \n",
       "\n",
       "[2302 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweepf_valid = dh.read_csv_file(\"data/tweepfake/validation.csv\")\n",
    "tweepf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96637700-c0b9-437e-9849-1afbe097595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnn</td>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>markov</td>\n",
       "      <td>436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-2</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  count\n",
       "0   human   1150\n",
       "1     rnn    370\n",
       "2  markov    436\n",
       "3   gpt-2    346"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_count = tweepf_valid[tweepf_valid.class_type==\"human\"].text.count()\n",
    "rnn_count = tweepf_valid[tweepf_valid.class_type==\"rnn\"].text.count()\n",
    "oth_count = tweepf_valid[tweepf_valid.class_type==\"others\"].text.count()\n",
    "gpt2_count = tweepf_valid[tweepf_valid.class_type==\"gpt2\"].text.count()\n",
    "pd.DataFrame({\"type\": [\"human\", \"rnn\", \"markov\", \"gpt-2\"], \"count\": [human_count, rnn_count, oth_count, gpt2_count]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "468616ed-2962-495b-95d2-90de7eeaa306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>account.type</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zawvrk</td>\n",
       "      <td>justin timberlake really one of the goats if y...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>narendramodi</td>\n",
       "      <td>Thank you @PMBhutan for your gracious prayers ...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahadsheriff</td>\n",
       "      <td>Theory: the number of red lights you will hit ...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AINarendraModi</td>\n",
       "      <td>Respects on the Upt of the I good with the peo...</td>\n",
       "      <td>bot</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kevinhooke</td>\n",
       "      <td>Might give the BASIC #10Liner game contest ano...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>ahadsheriffbot</td>\n",
       "      <td>“The best kept secret</td>\n",
       "      <td>bot</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>kevinhooke</td>\n",
       "      <td>Love the Choose your own adventure style of th...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>dril_gpt2</td>\n",
       "      <td>JOIN OUR TEAM: Sneezing</td>\n",
       "      <td>bot</td>\n",
       "      <td>gpt2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>kevinhooke</td>\n",
       "      <td>These deeply discounted 256GB SanDisk flash dr...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>zawvrk</td>\n",
       "      <td>@deleonfc6 diamond pearl and platinum were hon...</td>\n",
       "      <td>human</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2558 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         screen_name                                               text  \\\n",
       "0             zawvrk  justin timberlake really one of the goats if y...   \n",
       "1       narendramodi  Thank you @PMBhutan for your gracious prayers ...   \n",
       "2        ahadsheriff  Theory: the number of red lights you will hit ...   \n",
       "3     AINarendraModi  Respects on the Upt of the I good with the peo...   \n",
       "4         kevinhooke  Might give the BASIC #10Liner game contest ano...   \n",
       "...              ...                                                ...   \n",
       "2553  ahadsheriffbot                              “The best kept secret   \n",
       "2554      kevinhooke  Love the Choose your own adventure style of th...   \n",
       "2555       dril_gpt2                            JOIN OUR TEAM: Sneezing   \n",
       "2556      kevinhooke  These deeply discounted 256GB SanDisk flash dr...   \n",
       "2557          zawvrk  @deleonfc6 diamond pearl and platinum were hon...   \n",
       "\n",
       "     account.type class_type  \n",
       "0           human      human  \n",
       "1           human      human  \n",
       "2           human      human  \n",
       "3             bot        rnn  \n",
       "4           human      human  \n",
       "...           ...        ...  \n",
       "2553          bot     others  \n",
       "2554        human      human  \n",
       "2555          bot       gpt2  \n",
       "2556        human      human  \n",
       "2557        human      human  \n",
       "\n",
       "[2558 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweepf_test = dh.read_csv_file(\"data/tweepfake/test.csv\")\n",
    "tweepf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d988785-fa0b-43f2-be8b-965390bf3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_text = list(tweepf_valid[tweepf_valid.text.str.len() < 512].text)\n",
    "valid_label = list(tweepf_valid[tweepf_valid.text.str.len() < 512][\"account.type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84e8c67-93c4-4c93-9fca-63636752372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.00\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "predictions = [\n",
    "    # get_prediction(x) for x in pipe(valid_text)\n",
    "]\n",
    "# predictions[:5]\n",
    "\n",
    "end = time.time()\n",
    "print(f\"elapsed time: {end - start:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed073aa2-6abf-4831-9ed6-a0e069b4da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_edit = [\"human\" if x[\"label\"] == \"Human\" else \"bot\" for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d01820ab-75dd-49dd-a28e-8b2403f8941b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total out of the box accuracy:  0.00%\n"
     ]
    }
   ],
   "source": [
    "num_correct = sum(x == y for x, y in zip(valid_label, pred_edit))\n",
    "accuracy = num_correct / len(valid_text)\n",
    "print(f\"total out of the box accuracy: {accuracy * 100: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "198e1186-52bc-4f94-8a74-9cdce45abfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = set([x if z[0] == z[1] else None for x, z in enumerate(zip(valid_label, pred_edit))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f8dcacd-b8d6-4998-ad88-5e587026b780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for just GPT-2 samples: 0.00%\n"
     ]
    }
   ],
   "source": [
    "num_gpt2 = tweepf_valid[\n",
    "    (tweepf_valid.index.isin(indices)) & (tweepf_valid.text.str.len() < 512) & (tweepf_valid.class_type == \"gpt2\")\n",
    "].text.count()\n",
    "\n",
    "tot_gpt2 = tweepf_valid[tweepf_valid.class_type == \"gpt2\"].text.count()\n",
    "print(f\"accuracy for just GPT-2 samples: {num_gpt2*100/tot_gpt2:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c782113-a946-40dc-a59a-09f7c49d9b79",
   "metadata": {},
   "source": [
    "out of the box - completely random at 50%\n",
    "\n",
    "and surprisingly, does really poorly on GPT2 as well even though it was trained on a chatGPT corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61202254-9682-4898-9444-a432d82e9eb2",
   "metadata": {},
   "source": [
    "# Finetuning on TweepFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11ea8fd9-01b6-4bdc-ae7e-06e2a83fb0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 20712\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2302\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 2558\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# since we need 1's and 0's for training instead of text - this mapping needs to occur\n",
    "tweepf_train[\"account.type\"] = tweepf_train[\"account.type\"].map({\"human\": 0, \"bot\": 1})\n",
    "tweepf_valid[\"account.type\"] = tweepf_valid[\"account.type\"].map({\"human\": 0, \"bot\": 1})\n",
    "tweepf_test[\"account.type\"] = tweepf_test[\"account.type\"].map({\"human\": 0, \"bot\": 1})\n",
    "\n",
    "# reformat the pandas dfs into a Dataset for training\n",
    "# if you need the types account type - you can left join it back in on the text\n",
    "train_dataset = datasets.Dataset.from_dict(tweepf_train.drop([\"screen_name\", \"class_type\"], axis=1).rename(columns={\"account.type\": \"labels\"}))\n",
    "valid_dataset = datasets.Dataset.from_dict(tweepf_valid.drop([\"screen_name\", \"class_type\"], axis=1).rename(columns={\"account.type\": \"labels\"}))\n",
    "test_dataset = datasets.Dataset.from_dict(tweepf_test.drop([\"screen_name\", \"class_type\"], axis=1).rename(columns={\"account.type\": \"labels\"}))\n",
    "datasets = datasets.DatasetDict({\"train\": train_dataset, \"valid\": valid_dataset, \"test\": test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2e02df0-a8f1-482b-a946-37b69eac5d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(datasets[\"train\"][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "672fe05a-4ddd-458c-865d-2b2dba1541e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████| 20712/20712 [00:05<00:00, 3861.96 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████| 2302/2302 [00:00<00:00, 3820.29 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████| 2558/2558 [00:00<00:00, 3413.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_ds = datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65c4ded5-73a0-4946-9c10-0d5f97e04d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = tokenized_ds.remove_columns([\"text\"])\n",
    "small_train_dataset = tokenized_ds[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_valid_dataset = tokenized_ds[\"valid\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dee7a27-853f-4f15-bc08-714f9d0a7121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d0dda5b-0308-49b9-966f-0878b09c02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bce3ea9-906f-43a9-a64e-2523f89f2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=512)\n",
    "valid_dataloader = DataLoader(small_valid_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "953cdc2a-3a71-4254-b201-99431616a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0406ca68-2908-4069-a78b-01a6eaf26606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16809fe7-192e-423e-84f8-17548a0c0ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84928ff-f58b-4c22-9362-079ac6668df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: torch.from_numpy(np.asarray(v)).to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: torch.from_numpy(np.asarray(v)).to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    \n",
    "    metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fe7b27-9581-41f8-9702-e15455399c11",
   "metadata": {},
   "source": [
    "# Calculating Tweet Parse Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326a427-3730-4a08-aa81-251bef5360b0",
   "metadata": {},
   "source": [
    "# Calculating Tweet Perplexity\n",
    "you need to do this with a model trained on twitter data - not a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccdf3b-fd64-4974-a8c3-231b8093e430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
